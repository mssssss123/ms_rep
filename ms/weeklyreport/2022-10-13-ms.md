# 2022-10-13

# Paper Reading
|论文|总结|
|--|:-----|
|Augmented SBERT: Data Augmentation Method for ImprovingBi-Encoders for Pairwise Sentence Scoring Tasks|cross-encoder的缺点是效率低而bi-encoder训练需要大量训练数据。本文提出一种数据增强方式，即用较少的有标签数据去训练好一个cross-encoder的模型，并用训练好的模型去对无标签的数据打标签用于训练bi-encoder模型|
|Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks|bert对句子对的回归任务采用cross-encoder，但这样做效率太低了，本文的做法其实是预先对句子进行编码，当query到来只需要对query进行编码|
|session-based recommendations with recurrent neural networks|本文对序列化推荐采取in-batch negative给出了解释：用户大概率是因为不知道该商品而没有交互而不是知道了因为不喜欢，用户交互的商品大多是流行的商品，因此对于一个mini-batch内其他用户交互而自身没交互的是用户知道但不喜欢的概率更高|
|Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing|这周学习了这篇prompt的综述|
|TextBox: A Unified, Modularized, and ExtensibleFramework for Text Generation|本文介绍了一个统一的文本生成任务的库，实现了21个文本生成模型，使用和recbole一样|

# Experiment

1.只使用商品名字作为文本信息
|模型|Recall@10|Recall@20|NDCG@10|NDCG@20|
|-----|-----|-----|-----|----|
|openmatch-name(ours)|0.0236|0.0356|0.0131|0.0161|
|t5-name(ours)|0.0268|0.0381|0.0152|0.0180|
|GRU4Rec(复现)|0.0350|0.0585|0.0177|0.0236|
|GRU4Rec(论文)|0.0361|0.0592|0.0184|0.0243|

2.openmatch使用的scaled之后的t5，开了fp16.我测试了多个学习率，目前选择的是5e-6，但这个也有些抖，其他学习率的对比结果附在链接中.
<https://docs.google.com/spreadsheets/d/1kYLvqt7wCsbr_9SKy2WCzkQiWOygT1xbzOzElqueaic/edit#gid=0>

3.学习率5e-6的tensorboard图像如下
<img width="1074" alt="截屏2022-10-12 18 54 38" src="https://user-images.githubusercontent.com/56372416/195359890-a6f44a0d-d520-43b2-b517-80eef30566d0.png">
<img width="738" alt="截屏2022-10-12 18 55 08" src="https://user-images.githubusercontent.com/56372416/195360049-9fb7bbc2-f5a2-40cb-bfd7-9074eab1eee6.png">
